
1. Import spark 2.2.0 source code into IntelliJ
Import as a mvn project.
Search and find JavaWordCount.java, and try to run it to make the whole project compiled.

or just import the project by selecting the pom.xml

2. Update spark-examples_2.11.iml to include RUNTIME support for dependency
Yes, in latest version of IntelliJ, change dependency from "Provided" to "Compile" because by default, it is assuming you run spark-submit to execute the app.

3. spark-streaming-flume-sink_2.11
Need some special setup for this module in IntelliJ
target can NOT be excluded completely
treat target as Souces, especially external\flume-sink\target\scala-2.11\src_managed\main\compiled_avro\org\apache\spark
# No need to do this step in my second try.

4. config for JavaWordCount
org.apache.spark.examples.JavaWordCount
-Dlog4j.configuration=file:/home/zbhuang/BigDataDev/log4j.properties -Dspark.master=local[2]
# Root logger option
# ERROR/WARN/INFO
log4j.rootLogger=ERROR, stdout

# Direct log messages to stdout
log4j.appender.stdout=org.apache.log4j.ConsoleAppender
log4j.appender.stdout.Target=System.out
log4j.appender.stdout.layout=org.apache.log4j.PatternLayout
log4j.appender.stdout.layout.ConversionPattern=%d{yyyy-MM-dd HH:mm:ss} %-5p %c{1}:%L - %m%n

5. Check DAG
By adding the following code at the end, you have time to check the DAG through http://192.168.99.1:4040

        try {
            Thread.sleep(5_000_000);
        } catch (InterruptedException ex) {
            System.out.println(ex.getMessage());
        }

Reference
1. import Spark source code into intellj, build Error: not found: type SparkFlumeProtocol and EventBatch
https://stackoverflow.com/questions/33311794/import-spark-source-code-into-intellj-build-error-not-found-type-sparkflumepr

2. JavaWordCount.java
C:\Javax64\jdk1.8.0_144\bin\java -Dspark.master=local[2] -Xms512m -Xmx1024m -XX:MaxPermSize=300m -ea "-javaagent:C:\Program Files\JetBrains\IntelliJ IDEA Community Edition 2017.2.3\lib\idea_rt.jar=54272:C:\Program Files\JetBrains\IntelliJ IDEA Community Edition 2017.2.3\bin" -Dfile.encoding=UTF-8 -classpath ... org.apache.spark.examples.JavaWordCount inFiles outFiles
Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
17/10/18 08:37:41 INFO SparkContext: Running Spark version 2.2.0
17/10/18 08:37:42 INFO SparkContext: Submitted application: JavaWordCount
17/10/18 08:37:42 INFO SecurityManager: Changing view acls to: Zhibin
17/10/18 08:37:42 INFO SecurityManager: Changing modify acls to: Zhibin
17/10/18 08:37:42 INFO SecurityManager: Changing view acls groups to: 
17/10/18 08:37:42 INFO SecurityManager: Changing modify acls groups to: 
17/10/18 08:37:42 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(Zhibin); groups with view permissions: Set(); users  with modify permissions: Set(Zhibin); groups with modify permissions: Set()
17/10/18 08:37:42 INFO Utils: Successfully started service 'sparkDriver' on port 54315.
17/10/18 08:37:42 INFO SparkEnv: Registering MapOutputTracker
17/10/18 08:37:42 INFO SparkEnv: Registering BlockManagerMaster
17/10/18 08:37:42 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
17/10/18 08:37:42 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
17/10/18 08:37:42 INFO DiskBlockManager: Created local directory at C:\Users\Zhibin\AppData\Local\Temp\blockmgr-5032ece7-1515-40df-825c-830f30df1df7
17/10/18 08:37:42 INFO MemoryStore: MemoryStore started with capacity 366.3 MB
17/10/18 08:37:42 INFO SparkEnv: Registering OutputCommitCoordinator
17/10/18 08:37:42 INFO log: Logging initialized @1999ms
17/10/18 08:37:42 INFO Server: jetty-9.3.11.v20160721
17/10/18 08:37:42 INFO Server: Started @2056ms
17/10/18 08:37:42 INFO AbstractConnector: Started ServerConnector@21d03963{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
17/10/18 08:37:42 INFO Utils: Successfully started service 'SparkUI' on port 4040.
17/10/18 08:37:42 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@1ec9bd38{/jobs,null,AVAILABLE,@Spark}
...
17/10/18 08:37:42 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@5f77d0f9{/stages/stage/kill,null,AVAILABLE,@Spark}
17/10/18 08:37:42 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://192.168.99.1:4040
17/10/18 08:37:42 INFO Executor: Starting executor ID driver on host localhost
17/10/18 08:37:42 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 54332.
17/10/18 08:37:42 INFO NettyBlockTransferService: Server created on 192.168.99.1:54332
17/10/18 08:37:42 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
17/10/18 08:37:42 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 192.168.99.1, 54332, None)
17/10/18 08:37:42 INFO BlockManagerMasterEndpoint: Registering block manager 192.168.99.1:54332 with 366.3 MB RAM, BlockManagerId(driver, 192.168.99.1, 54332, None)
17/10/18 08:37:42 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 192.168.99.1, 54332, None)
17/10/18 08:37:42 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 192.168.99.1, 54332, None)
17/10/18 08:37:42 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@4ff4357f{/metrics/json,null,AVAILABLE,@Spark}
17/10/18 08:37:43 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('file:/K:/HadoopDev/spark220mvn/spark-warehouse/').
17/10/18 08:37:43 INFO SharedState: Warehouse path is 'file:/K:/HadoopDev/spark220mvn/spark-warehouse/'.
...
17/10/18 08:37:43 INFO StateStoreCoordinatorRef: Registered StateStoreCoordinator endpoint
java.lang.AssertionError: assertion failed: unsafe symbol Unstable (child of package InterfaceStability) in runtime reflection universe
	at scala.reflect.internal.Symbols$Symbol.<init>(Symbols.scala:184)
	at scala.reflect.internal.Symbols$TypeSymbol.<init>(Symbols.scala:3009)
	at org.apache.spark.sql.DataFrameReader.textFile(DataFrameReader.scala:632)
	...
Exception in thread "main" java.lang.RuntimeException: error reading Scala signature of org.apache.spark.sql.package: assertion failed: unsafe symbol Unstable (child of package InterfaceStability) in runtime reflection universe
	at scala.reflect.internal.pickling.UnPickler.unpickle(UnPickler.scala:46)
	at scala.reflect.runtime.JavaMirrors$JavaMirror.unpickleClass(JavaMirrors.scala:619)
	at scala.reflect.runtime.SymbolLoaders$TopClassCompleter$$anonfun$complete$1.apply$mcV$sp(SymbolLoaders.scala:28)
	...
17/10/18 08:37:48 INFO SparkContext: Invoking stop() from shutdown hook
17/10/18 08:37:48 INFO AbstractConnector: Stopped Spark@21d03963{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
17/10/18 08:37:48 INFO SparkUI: Stopped Spark web UI at http://192.168.99.1:4040
17/10/18 08:37:48 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
17/10/18 08:37:48 INFO MemoryStore: MemoryStore cleared
17/10/18 08:37:48 INFO BlockManager: BlockManager stopped
17/10/18 08:37:48 INFO BlockManagerMaster: BlockManagerMaster stopped
17/10/18 08:37:48 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
17/10/18 08:37:48 INFO SparkContext: Successfully stopped SparkContext
17/10/18 08:37:48 INFO ShutdownHookManager: Shutdown hook called
17/10/18 08:37:48 INFO ShutdownHookManager: Deleting directory C:\Users\Zhibin\AppData\Local\Temp\spark-b327acb6-15cd-47eb-818a-b7aa5363f3cd
Java HotSpot(TM) 64-Bit Server VM warning: ignoring option MaxPermSize=300m; support was removed in 8.0

Process finished with exit code 1

https://stackoverflow.com/questions/53274376/spark-wordcount-assertion-failed-unsafe-symbol-unstable
It turns out that this issue, unsafe symbol Unstable, is because of that JavaWordCount.java.
If it is written in different way, then no such issue.

3. Updated version of WordCount.java
------------------------------------
This version does not the previous unsafe symbol Unstable issue.

package org.apache.spark.examples;

import org.apache.spark.SparkConf;
import org.apache.spark.api.java.JavaPairRDD;
import org.apache.spark.api.java.JavaRDD;
import org.apache.spark.api.java.JavaSparkContext;
import org.apache.spark.api.java.function.FlatMapFunction;
import org.apache.spark.api.java.function.Function2;
import org.apache.spark.api.java.function.PairFunction;
import org.apache.spark.api.java.function.VoidFunction;
import scala.Tuple2;

import java.util.Arrays;
import java.util.Iterator;

public class WordCountLocal {

    private static final String OUTPUTFILEPATH = "./data/wordcountoutput";

    public static void main(String[] args) {
        /*
         第一步：创建SparkConf对象，设置Spark应用的配置信息
         使用setMaster()可以设置Spark应用程序要连接的Spark集群的master节点的url
         设置为local则代表在本地运行*/
        SparkConf conf = new SparkConf().setAppName("WordCountLocal").setMaster("local");

        // 第二步： 创建SparkContext对象【详细注解见博客：“注释i”】
        JavaSparkContext sc = new JavaSparkContext(conf);

        /*
         第三步：要针对输入源（hdfs文件、本地文件，等等），创建一个初始的RDD，
         [我们有两种方法创建RDD：
         1. 读取一个外部数据集(比如这里的本地文件);
         2. 在驱动程序里分发驱动器程序中的集合(如list, set等)]。
         输入源中的数据会打乱，分配到RDD的每个partition中，从而形成一个初始的分布式的数据集
         在Java中，创建的普通RDD，都叫做JavaRDD
         RDD中，有元素这种概念，如果是hdfs或者本地文件创建的RDD，每一个元素就相当于是文件里的一行*/
        // textFile()用于根据文件类型的输入源创建RDD
        JavaRDD<String> lines = sc.textFile("./data/wordcount.txt");

        /*
         第四步：对初始RDD进行transformation操作，也就是一些计算操作
         注意，RDD支持两种类型的操作，一种是转化（transformation）操作，一种是行动（action）操作。【详细注解见博客：“注释ii”】
        */
        // flatMap将RDD的一个元素给拆分成多个元素；FlatMapFunction的两个参数分别是输入和输出类型
        JavaRDD<String> words = lines.flatMap(new FlatMapFunction<String, String>() {
            @Override
            public Iterator<String> call(String line) throws Exception {
                return Arrays.asList(line.split(" ")).iterator();
            }
        });

        // 第五步：将每一个单词，映射为(单词, 1)的这种格式。为之后进行reduce操作做准备。
        // mapToPair，就是将每个元素映射为一个(v1,v2)这样的Tuple2（scala里的Tuple类型）类型的元素
        // mapToPair得与PairFunction配合使用，PairFunction中的第一个泛型参数代表输入类型
        // 第二个和第三个泛型参数，代表的输出的Tuple2的第一个值和第二个值的类型
        // JavaPairRDD的两个泛型参数，分别代表了tuple元素的第一个值和第二个值的类型

//        JavaPairRDD<String, Integer> pairs = words.mapToPair(
//                new PairFunction<String, String, Integer>() {
//                    private static final long serialVersionUID = 1L;
//                    @Override
//                    public Tuple2<String, Integer> call(String word) throws Exception {
//                        return new Tuple2<>(word, 1);
//                    }});

        // 注：mapToPair的lambda版本：【详见注释"iii"】
        JavaPairRDD<String, Integer> pairs = words.mapToPair(
            (PairFunction<String, String, Integer>) word -> new Tuple2(word, 1)
        );

        // 第六步：reduce操作（原理同MapReduce的reduce操作一样）
        // 以单词作为key，统计每个单词出现的次数
        // 最后返回的JavaPairRDD中的元素，也是tuple，但是第一个值就是每个key，第二个值就是key出现的次数
//        JavaPairRDD<String, Integer> wordCounts = pairs.reduceByKey(
//                new Function2<Integer, Integer, Integer>() {
//                    private static final long serialVersionUID = 1L;
//                    @Override
//                    public Integer call(Integer v1, Integer v2) throws Exception {
//                        return v1 + v2;
//                    }});

        JavaPairRDD<String, Integer> wordCounts = pairs.reduceByKey(
            (Function2<Integer, Integer, Integer>) (x, y) -> (x + y)
        );

        // 第七步：触发程序执行
        // 到这里为止，我们通过几个Spark算子操作，已经统计出了单词的次数
        // 目前为止我们使用的flatMap、mapToPair、reduceByKey操作，都是transformation操作
        // 现在我们需要一个action操作来触发程序执行（这里是foreach）
        wordCounts.foreach(new VoidFunction<Tuple2<String, Integer>>() {
            private static final long serialVersionUID = 1L;

            @Override
            public void call(Tuple2<String, Integer> wordCount) throws Exception {
                System.out.println(wordCount._1 + " is showing up " + wordCount._2 + " time;");
            }
        });

        // 我们也可以通过将统计出来的结果存入文件（这也是一个action操作），来触发之前的transformation操作
        try {
            wordCounts.saveAsTextFile(OUTPUTFILEPATH);
        } catch (Exception e) {
            e.printStackTrace();
        }

        try {
            Thread.sleep(5_000_000);
        } catch (InterruptedException ex) {
            System.out.println(ex.getMessage());
        }

        sc.close();
    }
}
